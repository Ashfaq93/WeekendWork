# randomWork

Deep feedword neural networks

'''
input layer -> weights -> hidden layer -> activation -> weights -> layer l2 -> activation -> weights ->  output layer

cost function = predicted out to intended ouput (cross entropy)
optimization function (optimizer) -> minimizing cost (adagrad, adamoptimizer, sgd)

back prop

feed forward + backprop = epoch
'''
